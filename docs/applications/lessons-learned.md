# Lessons Learned: Building Finbot

**Document:** Interview Preparation — Reflective Lessons
**Context:** Medical school application and interview preparation
**Date:** 2026-02-17

This document captures 15+ concrete lessons learned from three years of building Finbot. Each lesson includes what happened, what I learned, and how it applies beyond the specific technical context.

---

## Technical Challenges

### Lesson 1: Simple Beats Clever

**What happened:** Early Finbot used numba `@jit` decorators to JIT-compile Python to native code for performance. It looked impressive in benchmarks. Then Python 3.12 dropped support for a numba dependency, breaking the entire pipeline.

**What I learned:** Adding a C-extension dependency for performance gains requires the dependency to outlive your code. Vectorized NumPy was already fast enough — the numba optimizations provided a marginal improvement at the cost of brittleness, debugging difficulty, and Python version lock-in.

**The lesson:** Optimize for maintainability first; optimize for performance only when you've measured that it's actually a bottleneck. "Good enough and simple" consistently outperforms "perfect and complex" over long time horizons. This is as true in clinical practice as in software — the diagnostic approach that reliably works beats the elegant approach that occasionally fails.

---

### Lesson 2: Test Before You Trust

**What happened:** I migrated a strategy from Backtrader to NautilusTrader expecting the results to match. They didn't. After two hours of debugging, I found a subtle difference in how the two engines handle the first bar of data — Backtrader skips it; Nautilus processes it. One bar's difference in entry timing, compounded over 40 years of simulation, produced a 3% difference in CAGR.

**What I learned:** "It works" and "it's correct" are different claims. I built a parity testing framework (golden strategy regression tests) that runs both engines on the same data and asserts that CAGR, Sharpe ratio, and max drawdown match within 1 basis point. Now I can't break parity by accident.

**The lesson:** Trust but verify. In medicine, this is the principle behind diagnostic confirmation, second opinions, and critical appraisal of evidence. Automated verification (tests, checklists, protocols) is what scales trust from individuals to systems.

---

### Lesson 3: Name Things for What They Are

**What happened:** I had a function called `_process_data()` that actually fetched prices from Yahoo Finance, normalized column names, filled missing values, converted to parquet, and cached the result. It was called in 12 places. When the caching logic needed to change, I had to trace all 12 call sites.

**What I learned:** A function that does five things should be five functions with accurate names. `fetch_yfinance_prices()`, `normalize_price_columns()`, `fill_missing_prices()`, `cache_as_parquet()`. Now each piece is independently testable, independently replaceable, and self-documenting.

**The lesson:** Clarity in naming is clarity in thinking. A diagnosis that says "patient has chest pain" doesn't help; a diagnosis that says "patient has pleuritic chest pain worse on inspiration with a recent URI, most consistent with pleuritis" enables action. Precision in language reflects precision in understanding.

---

### Lesson 4: Pickle Is a Reliability Liability

**What happened:** Python 3.11 couldn't read pickle files generated by Python 3.9. Every time the Python version changed, cached data became unreadable. This happened three times before I replaced pickle with parquet.

**What I learned:** Data formats are contracts between the code that writes them and the code that reads them. Formats that break across versions, without warning, at read time, are professionally indefensible for long-lived systems. Parquet is self-describing, version-stable, and cross-language.

**The lesson:** Choose infrastructure that ages well. In healthcare: don't store patient records in proprietary formats that are locked to a single vendor. Interoperability standards (HL7 FHIR, DICOM) exist precisely because data outlives the systems that created it.

---

### Lesson 5: Testing Is Investment, Not Overhead

**What happened:** Early Finbot had 18 tests. Refactoring anything was terrifying — I couldn't tell what I'd broken without running the whole system manually. Once I built out to 866 tests, I made a major architectural change (switching from pickle to parquet) across 40+ files in a single afternoon. The tests caught two edge cases I'd missed.

**What I learned:** Tests aren't bureaucratic overhead — they're the infrastructure that makes confident change possible. The time I "saved" by not writing tests early was paid back tenfold in fearful, slow, manual validation later.

**The lesson:** Investment in system safety pays compound returns. Quality infrastructure (tests, protocols, checklists) seems expensive when the system is working. It becomes indispensable when something breaks.

---

## Collaboration and Process

### Lesson 6: Documentation Is a Gift to Your Future Self

**What happened:** I looked at code I'd written 18 months earlier and had no idea why it worked the way it did. A function computed an approximation to the overnight LIBOR rate using a piecewise formula with magic numbers. The code worked — I had results to prove it — but I couldn't explain *why* the formula was correct.

**What I learned:** Documentation isn't for other people. It's for the version of yourself who wrote the code and no longer remembers what you were thinking. Every architectural decision now has an ADR. Every algorithm has a docstring explaining the mathematical basis. The overnight LIBOR function now has a comment explaining which Federal Reserve paper the piecewise approximation comes from.

**The lesson:** When you document your reasoning, you force yourself to articulate it clearly. Clear articulation reveals gaps. In clinical practice: writing clear patient notes isn't just a communication obligation — it's a thinking discipline that catches errors.

---

### Lesson 7: Failing Loudly Beats Failing Silently

**What happened:** A data collection function was silently swallowing API errors and returning None. Downstream code treated None as valid data, producing results that were wrong in subtle ways — not obviously wrong, just slightly off. I spent a week convinced the strategy was underperforming before tracing the bug to a silent API failure from four days earlier.

**What I learned:** Code that fails silently is worse than code that crashes loudly. A crash at the point of failure tells you exactly what went wrong. A silent failure propagates incorrect state until it surfaces as an inexplicable result far from the root cause.

**The lesson:** Errors should be loud and immediate. In medicine: an adverse drug reaction that presents subtly three weeks after prescription is harder to trace than one that presents immediately. Design systems to make failures visible.

---

### Lesson 8: Small Batch Sizes Beat Big Bang Releases

**What happened:** I once accumulated 3 weeks of changes before committing. When I finally committed, one subtle change had broken 12 tests — but I couldn't tell which change caused it. I spent half a day doing a manual binary search through uncommitted changes.

**What I learned:** Commit early and often. Small, focused commits are independently reviewable, independently reversible, and independently testable. The overhead of a commit is seconds; the overhead of debugging a large batch of changes is hours.

**The lesson:** Incremental progress with regular checkpoints beats long periods of unvalidated work. In surgical training: practicing sutures in simulation before operating on patients, then progressively increasing case complexity, is safer than attempting a complex procedure after only observing. Small batches enable faster learning and earlier error detection.

---

### Lesson 9: The Map Is Not the Territory

**What happened:** I built a walk-forward analysis framework to test whether strategies that worked in-sample also worked out-of-sample. The results were sobering: strategies that showed 15% CAGR in the backtesting window often showed 8% CAGR in the subsequent validation window. The backtest results were accurate — but they weren't predictive.

**What I learned:** Historical performance is evidence, not prediction. A strategy that would have worked in the past might not work in the future if the market regime that produced those returns changes. Walk-forward analysis partially addresses this, but it doesn't eliminate the fundamental limitation of using history as a guide.

**The lesson:** Models are simplifications of reality. A randomized controlled trial is an accurate model of the trial population under trial conditions — generalizing to your specific patient requires judgment about how similar they are to trial participants. The model tells you something true; it doesn't tell you everything relevant.

---

### Lesson 10: Complexity Has Carrying Costs

**What happened:** I built 12 backtesting strategies. After all that work, the buy-and-hold strategy consistently performed as well as or better than the complex momentum strategies over 20+ year periods. The complex strategies required constant attention: monitoring for regime changes, recalibrating parameters, handling edge cases.

**What I learned:** Complexity is only worth the carrying cost if it provides commensurate value. A system you can understand and maintain is more reliable than a system that performs marginally better in testing but behaves unexpectedly in production.

**The lesson:** The simplest treatment that reliably achieves the therapeutic goal is usually preferable to a complex regimen with marginal additional efficacy and substantial adherence burden. First-line treatments exist for a reason.

---

## Decision-Making

### Lesson 11: Make Reversible Decisions Quickly, Irreversible Decisions Slowly

**What happened:** Choosing a Python linter (ruff vs flake8) was a reversible decision — it could be changed later with a few hours of work. I made that decision in 10 minutes. Choosing the data serialization format (parquet vs SQLite vs pickle) was much harder to reverse — it would require migrating all existing data files. I spent two days researching before choosing parquet.

**What I learned:** Decision cost should scale with decision reversibility. Spending hours agonizing over a reversible choice is waste; spending minutes on an irreversible choice is risk.

**The lesson:** Triage decisions by consequence. In clinical settings: choosing a diagnostic test is generally more reversible than choosing a treatment. Choosing an antibiotic is generally more reversible than performing a surgery. Allocate deliberation proportionally.

---

### Lesson 12: Understand the Problem Before Building the Solution

**What happened:** I spent two weeks building a beautiful multi-threaded data fetcher with connection pooling, rate limiting, and retry logic. When I tested it end-to-end, I discovered the bottleneck wasn't the fetching — it was the disk I/O when saving thousands of small files. The multi-threading provided no improvement because the threads were all waiting on the same disk.

**What I learned:** Measure before optimizing. The problem I thought I had (slow network fetching) wasn't the actual bottleneck. The solution I built (multi-threading) didn't address the actual problem (disk I/O).

**The lesson:** Diagnose before prescribing. A patient presenting with fatigue could have anemia, hypothyroidism, depression, sleep apnea, or dozens of other causes. Prescribing iron before checking labs is occasionally right by luck; it's usually wrong and potentially harmful.

---

### Lesson 13: Defaults Shape Outcomes

**What happened:** Finbot's fund simulator default is to include management fees and bid-ask spreads in all simulations. This was a deliberate choice — the alternative (ignoring fees by default) would systematically produce optimistic results. Almost all users would use the default and never think about whether fees were included.

**What I learned:** Default settings are policy decisions. The default you choose determines the outcome for everyone who doesn't consciously choose otherwise, which is most people most of the time.

**The lesson:** Defaults in clinical care — default medication doses, default monitoring frequencies, default documentation templates — shape outcomes at scale. Designing good defaults is a high-leverage activity precisely because of how often the default is used without review.

---

## Growth and Reflection

### Lesson 14: Crediting Your Own Work Is Not Boasting

**What happened:** I significantly underestimated Finbot's scope in early descriptions — calling it "a backtesting library" when it had also become a health economics platform, an execution simulation system, and a data quality monitoring framework. This underselling made it harder to communicate the project's actual scope to others.

**What I learned:** Accurate communication requires accurate self-assessment. Underselling your work isn't humility — it's inaccuracy. Humility is acknowledging limitations and uncertainty; it doesn't require minimizing real accomplishments.

**The lesson:** In interviews and applications, describe your work accurately, including its scope and impact. Underselling out of false modesty creates the same misimpression as overselling. Both are forms of miscommunication.

---

### Lesson 15: The Questions That Look Closed Are Often Open

**What happened:** I started Finbot to answer "should I rebalance my portfolio?" I expected a clear yes/no. What I found after three years of analysis: it depends on the time period, the assets, the rebalancing frequency, the tax treatment, and several other factors — and the academic literature is genuinely divided on whether rebalancing adds value over long time horizons after costs.

**What I learned:** Many questions that appear to have correct answers are actually contested questions where the evidence is complex, contradictory, or limited. Acknowledging that complexity — rather than forcing false certainty — is the intellectually honest position.

**The lesson:** Medicine is full of questions that appear settled but aren't: when to screen, what thresholds to use, which treatment for which patient. Intellectual comfort with ambiguity — the ability to say "the evidence is genuinely mixed, and here's how we navigate that" — is a mark of clinical maturity.

---

## Summary

| Lesson | Domain | Core principle |
|--------|--------|---------------|
| Simple beats clever | Engineering/Clinical | Maintainability over elegance |
| Test before you trust | Quality | Verification over assumption |
| Name things accurately | Communication | Clarity reflects understanding |
| Choose formats that age well | Infrastructure | Design for longevity |
| Testing is investment | Process | Safety systems compound |
| Document for your future self | Communication | Writing forces clarity |
| Fail loudly | Systems | Visible errors are manageable |
| Small batches beat big bang | Process | Iteration beats accumulation |
| Models are not reality | Epistemology | Evidence has scope conditions |
| Complexity has carrying costs | Decision-making | Simplicity is a feature |
| Reversibility scales deliberation | Decision-making | Match effort to consequence |
| Diagnose before prescribing | Problem-solving | Understand before acting |
| Defaults shape outcomes | Design | Policy is embedded in architecture |
| Accurate self-assessment | Communication | Neither undersell nor oversell |
| Ambiguity is the honest position | Epistemology | Intellectual honesty about limits |

---

*This document supports medical school application and interview preparation. For project documentation, see the [Finbot repository](https://github.com/jerdaw/finbot).*
